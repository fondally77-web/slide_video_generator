import OpenAI from 'openai';
import fs from 'fs';

interface Segment {
    start: number;
    end: number;
    text: string;
}

// é–‹ç™ºãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
const isDevelopmentMode = !process.env.AZURE_OPENAI_API_KEY || !process.env.AZURE_OPENAI_ENDPOINT;

// Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
let client: OpenAI | null = null;
if (!isDevelopmentMode) {
    client = new OpenAI({
        apiKey: process.env.AZURE_OPENAI_API_KEY,
        baseURL: `${process.env.AZURE_OPENAI_ENDPOINT}/openai/deployments/${process.env.AZURE_OPENAI_WHISPER_DEPLOYMENT_NAME}`,
        defaultQuery: { 'api-version': '2024-02-15-preview' },
        defaultHeaders: { 'api-key': process.env.AZURE_OPENAI_API_KEY },
    });
}

/**
 * éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
 */
export async function transcribeAudio(audioPath: string): Promise<Segment[]> {
    // é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    if (isDevelopmentMode) {
        console.log('âš ï¸ é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰');
        // å°‘ã—å¾…æ©Ÿã—ã¦ãƒªã‚¢ãƒ«ãªå‡¦ç†æ„Ÿã‚’å‡ºã™
        await new Promise((resolve) => setTimeout(resolve, 2000));
        return [
            { start: 0, end: 15, text: 'ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚' },
            { start: 15, end: 30, text: 'äººå·¥çŸ¥èƒ½ã€ç•¥ã—ã¦AIã¯ã€äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚' },
            { start: 30, end: 45, text: 'æ©Ÿæ¢°å­¦ç¿’ã¯AIã®ä¸€åˆ†é‡ã§ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚' },
            { start: 45, end: 60, text: 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ©Ÿæ¢°å­¦ç¿’ã®ç™ºå±•å½¢ã§ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚' },
            { start: 60, end: 75, text: 'ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ã€ç”»åƒèªè­˜ã‚„è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚' },
        ];
    }

    try {
        console.log('ğŸ¤ éŸ³å£°èªè­˜ã‚’é–‹å§‹:', audioPath);

        const audioFile = fs.createReadStream(audioPath);

        const response = await client!.audio.transcriptions.create({
            file: audioFile,
            model: 'whisper-1',
            language: 'ja',
            response_format: 'verbose_json',
            timestamp_granularities: ['segment'],
        });

        console.log('âœ… éŸ³å£°èªè­˜å®Œäº†');

        // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
        // @ts-ignore - verbose_jsonå½¢å¼ã®è¿½åŠ ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
        const segments: Segment[] = (response.segments || []).map((seg: any) => ({
            start: seg.start,
            end: seg.end,
            text: seg.text.trim(),
        }));

        // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’1ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«
        if (segments.length === 0 && response.text) {
            segments.push({
                start: 0,
                end: 60,
                text: response.text.trim(),
            });
        }

        return segments;
    } catch (error) {
        console.error('âŒ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:', error);
        throw error;
    }
}
