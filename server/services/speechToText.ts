import { GoogleGenerativeAI } from '@google/generative-ai';
import OpenAI from 'openai';
import fs from 'fs';
import path from 'path';

interface Segment {
    start: number;
    end: number;
    text: string;
}

// ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const AZURE_OPENAI_API_KEY = process.env.AZURE_OPENAI_API_KEY;
const AZURE_OPENAI_ENDPOINT = process.env.AZURE_OPENAI_ENDPOINT;

// é–‹ç™ºãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆã©ã¡ã‚‰ã®APIã‚­ãƒ¼ã‚‚è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
const isDevelopmentMode = !GEMINI_API_KEY && (!AZURE_OPENAI_API_KEY || !AZURE_OPENAI_ENDPOINT);

// Azure OpenAI Whisper ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
let whisperClient: OpenAI | null = null;
if (AZURE_OPENAI_API_KEY && AZURE_OPENAI_ENDPOINT && process.env.AZURE_OPENAI_WHISPER_DEPLOYMENT_NAME) {
    whisperClient = new OpenAI({
        apiKey: AZURE_OPENAI_API_KEY,
        baseURL: `${AZURE_OPENAI_ENDPOINT}/openai/deployments/${process.env.AZURE_OPENAI_WHISPER_DEPLOYMENT_NAME}`,
        defaultQuery: { 'api-version': '2024-02-15-preview' },
        defaultHeaders: { 'api-key': AZURE_OPENAI_API_KEY },
    });
}

/**
 * éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ï¼ˆGeminiå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§Whisperï¼‰
 */
export async function transcribeAudio(audioPath: string): Promise<Segment[]> {
    // é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    if (isDevelopmentMode) {
        console.log('âš ï¸ é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: ãƒ¢ãƒƒã‚¯éŸ³å£°èªè­˜ã‚’ä½¿ç”¨');
        await new Promise((resolve) => setTimeout(resolve, 2000));
        return getMockSegments();
    }

    // Gemini APIã‚’å„ªå…ˆä½¿ç”¨
    if (GEMINI_API_KEY) {
        return transcribeWithGemini(audioPath);
    }

    // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Azure OpenAI Whisper
    if (whisperClient) {
        return transcribeWithWhisper(audioPath);
    }

    console.log('âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚');
    return getMockSegments();
}

/**
 * Gemini APIã§éŸ³å£°èªè­˜
 */
async function transcribeWithGemini(audioPath: string): Promise<Segment[]> {
    try {
        console.log('ğŸ¤ GeminiéŸ³å£°èªè­˜ã‚’é–‹å§‹:', audioPath);

        const genAI = new GoogleGenerativeAI(GEMINI_API_KEY!);
        const model = genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });

        // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        const audioData = fs.readFileSync(audioPath);
        const base64Audio = audioData.toString('base64');

        // ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰MIMEã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        const ext = path.extname(audioPath).toLowerCase();
        const mimeType = ext === '.wav' ? 'audio/wav' :
            ext === '.mp3' ? 'audio/mp3' :
                ext === '.m4a' ? 'audio/mp4' :
                    ext === '.webm' ? 'audio/webm' : 'audio/wav';

        const prompt = `ã“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãèµ·ã“ã—ã¦ãã ã•ã„ã€‚
JSONå½¢å¼ã§ã€ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{
  "segments": [
    {"start": 0, "end": 10, "text": "æœ€åˆã®æ–‡ç« "},
    {"start": 10, "end": 20, "text": "æ¬¡ã®æ–‡ç« "}
  ]
}

ãƒ«ãƒ¼ãƒ«ï¼š
- æ—¥æœ¬èªã§æ›¸ãèµ·ã“ã—ã¦ãã ã•ã„
- å¥èª­ç‚¹ã‚’é©åˆ‡ã«å…¥ã‚Œã¦ãã ã•ã„
- ç´„10-15ç§’ã”ã¨ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’åŒºåˆ‡ã£ã¦ãã ã•ã„
- startã¨endã¯ç§’æ•°ï¼ˆå°æ•°ç‚¹å¯ï¼‰
- JSONã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„`;

        const result = await model.generateContent([
            {
                inlineData: {
                    mimeType,
                    data: base64Audio,
                },
            },
            { text: prompt },
        ]);

        const response = await result.response;
        const text = response.text();

        // JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        const jsonMatch = text.match(/\{[\s\S]*\}/);
        if (!jsonMatch) {
            console.log('âš ï¸ JSONå½¢å¼ã®å¿œç­”ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚');
            return [{
                start: 0,
                end: 60,
                text: text.trim(),
            }];
        }

        const parsed = JSON.parse(jsonMatch[0]);
        const segments: Segment[] = parsed.segments || [];

        console.log('âœ… GeminiéŸ³å£°èªè­˜å®Œäº†:', segments.length, 'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ');
        return segments;

    } catch (error) {
        console.error('âŒ GeminiéŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:', error);
        return getMockSegments();
    }
}

/**
 * Azure OpenAI Whisperã§éŸ³å£°èªè­˜
 */
async function transcribeWithWhisper(audioPath: string): Promise<Segment[]> {
    try {
        console.log('ğŸ¤ WhisperéŸ³å£°èªè­˜ã‚’é–‹å§‹:', audioPath);

        const audioFile = fs.createReadStream(audioPath);

        const response = await whisperClient!.audio.transcriptions.create({
            file: audioFile,
            model: 'whisper-1',
            response_format: 'verbose_json',
            language: 'ja',
        });

        const segments: Segment[] = [];
        const responseAny = response as any;

        if (responseAny.segments) {
            for (const seg of responseAny.segments) {
                segments.push({
                    start: seg.start ?? 0,
                    end: seg.end ?? 0,
                    text: seg.text ?? '',
                });
            }
        } else if (responseAny.text) {
            segments.push({
                start: 0,
                end: 60,
                text: responseAny.text,
            });
        }

        console.log('âœ… WhisperéŸ³å£°èªè­˜å®Œäº†:', segments.length, 'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ');
        return segments;

    } catch (error) {
        console.error('âŒ WhisperéŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼:', error);
        return getMockSegments();
    }
}

/**
 * é–‹ç™ºç”¨ãƒ¢ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
 */
function getMockSegments(): Segment[] {
    return [
        { start: 0, end: 15, text: 'ã“ã‚“ã«ã¡ã¯ã€æœ¬æ—¥ã¯AIã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã™ã€‚' },
        { start: 15, end: 30, text: 'äººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤§ããå¤‰ãˆã¤ã¤ã‚ã‚Šã¾ã™ã€‚' },
        { start: 30, end: 45, text: 'ç‰¹ã«æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã®ç™ºå±•ãŒç›®è¦šã¾ã—ã„ã§ã™ã€‚' },
        { start: 45, end: 60, text: 'ä»Šå¾Œã‚‚AIæŠ€è¡“ã®é€²åŒ–ã«æ³¨ç›®ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚' },
    ];
}
